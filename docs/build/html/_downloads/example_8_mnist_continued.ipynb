{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample 8 - Warmstarting for MNIST\n==================================\n\nSometimes it is desired to continue an already finished run because the optimization\nrequires more function evaluations. In other cases, one might wish to use results\nfrom previous runs to speed up the optimization. This might be useful if initial\nruns were done with relatively small budgets, or on only a subset of the data to\nget an initial understanding of the problem.\n\nHere we shall see how to use the results from example 5 to initialize BOHB's model.\nWhat changed are\n- the number of training points is increased from 8192 to 32768\n- the number of validation points is increased from 1024 to 16384\n- the mimum budget is now 3 instead of 1 because we have already quite a few runs for a small number of epochs\n\nNote that the loaded runs will show up in the results of the new run. They are all\ncombined into an iteration with the index -1 and their time stamps are manipulated\nsuch that the last run finishes at time 0 with all other times being negative.\nThat info can be used to filter those runs when analysing the run.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport pickle\nimport argparse\n\nimport hpbandster.core.nameserver as hpns\nimport hpbandster.core.result as hpres\n\nfrom hpbandster.optimizers import BOHB\n\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\n\n\nparser = argparse.ArgumentParser(description='Example 1 - sequential and local execution.')\nparser.add_argument('--min_budget',   type=float, help='Minimum number of epochs for training.',    default=3)\nparser.add_argument('--max_budget',   type=float, help='Maximum number of epochs for training.',    default=9)\nparser.add_argument('--n_iterations', type=int,   help='Number of iterations performed by the optimizer', default=4)\nparser.add_argument('--worker', help='Flag to turn this into a worker process', action='store_true')\nparser.add_argument('--run_id', type=str, help='A unique run id for this optimization run. An easy option is to use the job id of the clusters scheduler.')\nparser.add_argument('--nic_name',type=str, help='Which network interface to use for communication.', default='lo')\nparser.add_argument('--shared_directory',type=str, help='A directory that is accessible for all processes, e.g. a NFS share.', default='.')\nparser.add_argument('--backend',help='Toggles which worker is used. Choose between a pytorch and a keras implementation.', choices=['pytorch', 'keras'], default='keras')\nparser.add_argument('--previous_run_dir',type=str, help='A directory that contains a config.json and results.json for the same configuration space.', default='./example_5_run/')\n\nargs=parser.parse_args()\n\n\nif args.backend == 'pytorch':\n\tfrom example_5_pytorch_worker import PyTorchWorker as worker\nelse:\n\tfrom example_5_keras_worker import KerasWorker as worker\n\n\n# Every process has to lookup the hostname\nhost = hpns.nic_name_to_host(args.nic_name)\n\n\nif args.worker:\n\timport time\n\ttime.sleep(5)\t# short artificial delay to make sure the nameserver is already running\n\tw = worker(run_id=args.run_id, host=host, timeout=120)\n\tw.load_nameserver_credentials(working_directory=args.shared_directory)\n\tw.run(background=False)\n\texit(0)\n\n\n# This example shows how to log live results. This is most useful\n# for really long runs, where intermediate results could already be\n# interesting. The core.result submodule contains the functionality to\n# read the two generated files (results.json and configs.json) and\n# create a Result object.\nresult_logger = hpres.json_result_logger(directory=args.shared_directory, overwrite=False)\n\n\n# Start a nameserver:\nNS = hpns.NameServer(run_id=args.run_id, host=host, port=0, working_directory=args.shared_directory)\nns_host, ns_port = NS.start()\n\n# Start local worker\nw = worker(run_id=args.run_id, host=host, nameserver=ns_host, nameserver_port=ns_port, timeout=120)\nw.run(background=True)\n\n\n# Let us load the old run now to use its results to warmstart a new run with slightly\n# different budgets in terms of datapoints and epochs.\n# Note that the search space has to be identical though!\nprevious_run = hpres.logged_results_to_HBS_result(args.previous_run_dir)\n\n\n# Run an optimizer\nbohb = BOHB(  configspace = worker.get_configspace(),\n\t\t\t  run_id = args.run_id,\n\t\t\t  host=host,\n\t\t\t  nameserver=ns_host,\n\t\t\t  nameserver_port=ns_port,\n\t\t\t  result_logger=result_logger,\n\t\t\t  min_budget=args.min_budget, max_budget=args.max_budget, \n\t\t\t  previous_result = previous_run,\t\t\t\t# this is how you tell any optimizer about previous runs\n\t\t   )\nres = bohb.run(n_iterations=args.n_iterations)\n\n# store results\nwith open(os.path.join(args.shared_directory, 'results.pkl'), 'wb') as fh:\n\tpickle.dump(res, fh)\n\n# shutdown\nbohb.shutdown(shutdown_workers=True)\nNS.shutdown()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}