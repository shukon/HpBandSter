{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nExample 3 - Warmstarting\n========================\n\nThis examples covers the warmstart functionality\nWe will start an optimizer run with a small budget\nThen we'll shutdown the master, but keep the nameserver alive.\nAnd finally, restarting the optimization run with a new master and a larger budget\n\nIn the end, we'll introduce an interactive visualization tool.\nWith this tool, we can illustrate the progress of the optimizer.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os\nimport pickle\nimport matplotlib.pyplot as plt\n\nimport ConfigSpace as CS\nimport logging\nlogging.basicConfig(level=logging.DEBUG)\n\nimport hpbandster.core.nameserver as hpns\nimport hpbandster.core.result as hpres\n\nfrom hpbandster.examples.commons import MyWorker, sample_configspace\nfrom hpbandster.optimizers import H2BO as opt\nfrom hpbandster.core.result import extract_HB_learning_curves\nfrom hpbandster.visualization import interactive_HB_plot, default_tool_tips\n\n\n# First, create a ConfigSpace-Object.\n# It contains the hyperparameters to be optimized\n# For more details, please have a look in the ConfigSpace-Example in the Documentation\nconfig_space = sample_configspace()\nconfig_space.add_hyperparameter(CS.UniformFloatHyperparameter('x', lower=0, upper=1))\n\n# We use live logging with the jason result logger\nresult_logger = hpres.json_result_logger(directory='.', overwrite=True)\n\n# Every run has to have a unique (at runtime) id.\n# This needs to be unique for concurrent runs, i.e. when multiple\n# instances run at the same time, they have to have different ids\nrun_id = '0'\n\n# Step 1:\n# Every run needs a nameserver. It could be a 'static' server with a\n# permanent address, but here it will be started for the local machine\n# with a random port.\n# The nameserver manages the concurrent running workers across all possible threads or clusternodes.\nNS = hpns.NameServer(run_id=run_id, host='localhost', port=0)\nns_host, ns_port = NS.start()\n\n# Step 2:\n# The worker implements the connection to the model to be evaluated.\n# Its 'compute'-method will be called later by the BOHB-optimizer repeatedly\n# with the sampled configurations and return for example the computed loss.\n# Further usages of the worker will be covered in a later example.\nworker = MyWorker(nameserver=ns_host, nameserver_port=ns_port, run_id=run_id)\nworker.run(background=True)\n\n# We will start the first run with a smaller budget, which we define here.\n# In the second run, we'll use three times as much.\nmin_budget = 9\nmax_budget = 243\n\n# Step 3:\n# The number of sampled configurations is determined by the\n# parameters eta, min_budget and max_budget.\n# After evaluating each configuration, starting with the minimum budget\n# on the same subset size, only a fraction of 1 / eta of them\n# 'advances' to the next round. At the same time the current budget will be doubled.\n# This process runs until the maximum budget is reached.\nHB = opt(   configspace=config_space,\n            run_id=run_id,\n            eta=3, min_budget=min_budget, max_budget=max_budget,\n            nameserver=ns_host,\n            nameserver_port=ns_port,\n            result_logger=result_logger,\n            min_points_in_model=4,\n            min_bandwidth=1e-1\n        )\n\n\n# Do a short run and shutdown the master (but keep the name server and the worker alive)\nres = HB.run(2)\nHB.shutdown(shutdown_workers=False)\n\n\n# Now let's start a new run, but feed in the results of the first one to warmstart the model.\n# Note that the budgets don't have to align, but beware: if the max budget of the second run is not\n# greater or equal to the max budget in the previous runs, BOHB's model might never be updated!\nmin_budget *= 3\nmax_budget *= 3\n\nHB = opt(   configspace=config_space,\n            run_id=run_id,\n            eta=3,min_budget=min_budget, max_budget=max_budget,\n            nameserver=ns_host,\n            nameserver_port=ns_port,\n            previous_result=res,  # Here is where we feed the previous run into our optimizer\n            min_points_in_model=4,\n            top_n_percent=5,\n            bandwidth_factor=1,\n            num_samples=32,\n            min_bandwidth=1e-1\n        )\nres = HB.run(4)\n\n# After the optimizer run, we shutdown the master.\nHB.shutdown(shutdown_workers=True)\nNS.shutdown()\n\n\n# BOHB will return a result object.\n# It holds informations about the optimization run like the incumbent (=best) configuration.\n# For further details about the result-object, see its documentation.\nid2config = res.get_id2config_mapping()\nprint('A total of %i unique configurations where sampled.'%len(id2config.keys()))\nprint('A total of %i runs where executed.'%len(res.get_all_runs()))\n\n\n# Hpbandster contains also a visualization tool to plot the\n# 'learning curves' of the sampled configurations\nincumbent_trajectory = res.get_incumbent_trajectory()\nlcs = res.get_learning_curves(lc_extractor=extract_HB_learning_curves)\n\ntool_tips = default_tool_tips(res, lcs)\nfig, ax, check, none_button, all_button = interactive_HB_plot(lcs, tool_tip_strings=tool_tips,\n                                                              show=False)\nax.set_ylim([0.1*incumbent_trajectory['losses'][-1], 1])\nax.set_yscale('log')\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}